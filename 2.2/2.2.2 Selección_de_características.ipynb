{"cells":[{"cell_type":"markdown","metadata":{"id":"M9z4zc76NLMv"},"source":["## Selección de Características"]},{"cell_type":"markdown","metadata":{"id":"Q694L1xhNLMx"},"source":["***\n","BASADO EN: https://www.kaggle.com/code/mojtylor/feature-selection-for-beginners\n","***"]},{"cell_type":"markdown","metadata":{"id":"bG-CcYIcNLMy"},"source":["Métodos de selección de características:\n","\n","Veremos 3 técnicas de selección de características que son fáciles de usar y también dan buenos resultados.\n","1. Selección univariada\n","2. Importancia de las características\n","3. Matriz de correlación con mapa de calor"]},{"cell_type":"markdown","metadata":{"id":"EdPoFeyNNLMy"},"source":["### **1. Selección Univariante**\n","\n","Las pruebas estadísticas se pueden utilizar para seleccionar aquellas características que tienen la relación más fuerte con la variable de salida.\n","\n","La biblioteca scikit-learn proporciona la clase SelectKBest que se puede usar con un conjunto de diferentes pruebas estadísticas para seleccionar una cantidad específica de funciones.\n","\n","El siguiente ejemplo utiliza la prueba estadística chi-cuadrado (chi²) para características no negativas, y busca seleccionar 10 de las mejores características del conjunto de datos de predicción de rango de precios móviles."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","id":"jcZBQ-j0NLMy"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.feature_selection import SelectKBest\n","from sklearn.feature_selection import chi2\n","data = pd.read_csv(\"sample_data/train.csv\")\n","X = data.iloc[:,0:20]  #independent columns\n","y = data.iloc[:,-1]    #target column i.e price range\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-PyWsnEfNLM0"},"outputs":[],"source":["data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N9xlJ4HeNLM0"},"outputs":[],"source":["data.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lynNbIl3NLM1"},"outputs":[],"source":["# apply SelectKBest class to extract top 10 best features\n","bestfeatures = SelectKBest(score_func=chi2 , k=10)\n","fit = bestfeatures.fit(X, y )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cq3UH3cBNLM1"},"outputs":[],"source":["dfscores = pd.DataFrame(fit.scores_)\n","dfcolumns = pd.DataFrame(X.columns)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gn4hQ53kNLM1"},"outputs":[],"source":["# concatenate two dataframes for better visualizaton\n","featureScores = pd.concat([dfcolumns , dfscores] ,axis=1)\n","featureScores.columns = ['Specs','Score']  #naming the dataframe columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KGNEUiayNLM1"},"outputs":[],"source":["featureScores"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Dxi82jMNLM1"},"outputs":[],"source":["featureScores.nlargest(10 , 'Score')"]},{"cell_type":"markdown","metadata":{"id":"HWRMyKFbNLM2"},"source":["\n","### **2. Importancia de las características**\n","Puede obtener la importancia de cada característica de su conjunto de datos utilizando la propiedad de importancia de la característica del modelo.\n","\n","La importancia le da una puntuación para cada característica de sus datos, cuanto mayor sea la puntuación, más importante o relevante es la característica para su variable de salida.\n","\n","La importancia de las características es una clase incorporada que viene con los clasificadores basados en árboles. Usaremos el clasificador de árboles extra para extraer las 10 características principales para el conjunto de datos."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"faqrAtBmNLM2"},"outputs":[],"source":["from sklearn.ensemble import ExtraTreesClassifier\n","import matplotlib.pyplot as plt\n","model = ExtraTreesClassifier()\n","model.fit(X,y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xCx2BdQBNLM2"},"outputs":[],"source":["print(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"64EIfIsqNLM2"},"outputs":[],"source":["#plot graph of feature importances for better visualization\n","feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n","feat_importances.nlargest(10).plot(kind='barh')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"DfA8NKZFNLM3"},"source":["### **3.Matriz de correlación con mapa de calor**\n","La correlación establece cómo se relacionan las características entre sí o con la variable de destino.\n","\n","La correlación puede ser positiva (el aumento de un valor de la función aumenta el valor de la variable objetivo) o negativa (el aumento de un valor de la función, disminuye el valor de la variable objetivo)\n","\n","El mapa de calor facilita la identificación de las características que están más relacionadas con la variable de destino. Trazaremos un mapa de calor de las características correlacionadas utilizando la biblioteca Seaborn."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rA1fEYRcNLM3"},"outputs":[],"source":["import seaborn as sns\n","#get correlations of each features in dataset\n","corrmat = data.corr()\n","top_corr_features = corrmat.index\n","plt.figure(figsize=(20,20))\n","#plot heat map\n","g=sns.heatmap(data[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")"]},{"cell_type":"markdown","source":["***\n","EJERCICIO: use el siguiente código para implementar el METODO DE FILTRO, con el fin de seleccionar las mejores características.\n","***"],"metadata":{"id":"ST0puo3dQDNh"}},{"cell_type":"code","source":["\n","def handling_correlation(df,threshold=0.8):\n","\n","    # creating set to hold the correlated features\n","\n","    corr_features = set()\n","\n","    # create the correlation matrix (default to pearson)\n","\n","    corr_matrix = df.corr()\n","\n","    #code for selecting all correleated features\n","\n","    for i in range(len(corr_matrix .columns)):\n","\n","        for j in range(i):\n","\n","            if abs(corr_matrix.iloc[i, j]) >threshold:\n","\n","                colname = corr_matrix.columns[i]\n","\n","                corr_features.add(colname)\n","\n","    return list(corr_features)"],"metadata":{"id":"tNpyJYjbQe4A"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"colab":{"provenance":[],"toc_visible":true}},"nbformat":4,"nbformat_minor":0}